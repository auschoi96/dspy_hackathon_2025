{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec10ebe-4240-4f6a-8ad7-495de1493f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Multi-PDF Parsing with LLama 4 Maverick and ThreadPool\n",
    "\n",
    "**Author: Erica Yuen**\n",
    "\n",
    "**Date: 07/03/2025**\n",
    "\n",
    "###Summary\n",
    "\n",
    "This notebook processes all PDFs in a Unity Catalog directory. It first converts the PDFs into individual pages as base 64 images, and then transcribes them with Llama 4, a best-in-class multimodal model that can caption figures and transcribe text and tables to markdown. This notebook automatically adjusts the number of workers using Threadpool and can be up to 2x faster than using a spark udf. This maximizes throughput, and outputs the progress of your files being processed.\n",
    "\n",
    "This uses Databricks Model Serving, which is also HIPAA compliant.\n",
    "\n",
    "### Notebook Instructions\n",
    "1. **Update the paths to your UC Schema, source PDFs, and workspace URL in [Section 2]** and run the subsequent cells\n",
    "\n",
    "\n",
    "2. **Optional:**\n",
    "    - Choose if you want to save your results in one table with `PROCESSING_MODE = combined` (recommended) or `separate`\n",
    "      - `PROCESSING_MODE = combined` will append the results in a single table (recommended). This will let you browse and query the results in Sections 5 and 6.\n",
    "      - `PROCESSING_MODE = separate` will separate the results in different tables, for large PDFs (1k+ pages) or if the tables need different permissions\n",
    "\n",
    "    - Update if you want `save_to_unity_catalog` `mode = \"append\"`(recommended) or `mode=\"overwrite\"`\n",
    "\n",
    "\n",
    "3. **Execute the batch processing in [Section 4]. Choose pay-per-token with `model=databricks-llama-4-maverick` or your own Provisioned Throughput Endpoint and update the number of initial, min, and max workers.**\n",
    "\n",
    "    - To create the Provisioned Throughput endpoint, go to **Catalog** > system.ai > Models > llama-4-maverick. Then click **Serve this Model**. \n",
    "\n",
    "    - _**Note:** Scale-to-zero is not available for Llama 4 model yet, so only provision what you need and delete the endpoint if you are not using it_\n",
    "\n",
    "    - Change the initial, max, and lowest workers based on your model endpoint. The code will automatically adapt the number of workers based on rate limits. For pay-per-token, 5-10 workers is a good start. For Provisioned Throughput 200 model units, use 30-40 workers. \n",
    "\n",
    "4. **Optional:**\n",
    "    - Query and Inspect the tables in Section 5 if `PROCESSING_MODE = combined`\n",
    "    - Browse the PDF page images and their respective transcribed outputs in Section 6\n",
    "\n",
    "### Speed Estimate \n",
    "Speed depends on your cluster configurations and the Llama 4 endpoint. \n",
    "Based on some example PDFs, here are the estimated times for processing files:\n",
    "- Converting PDF into base64 images: ~150 pages per min\n",
    "- Parsing pages with Llama 4 Maverick\n",
    "  - ~ 30 pages / min on Pay Per Token\n",
    "  - ~120 pages / min for Provisioned Throughput endpoint with 200 model units (scale to zero not available for now)\n",
    "  - So a PDF with 440 pages will take about 3 min preprocessing + 12 min parsing = 15 min on pay per token, or 6 min on PT endpoint with 200 model units\n",
    "\n",
    "###Price Estimate\n",
    "The price per page depends on the number of tokens each pages has. $3-7 dollars per 1k pages is estimate if you use pay per token, and if you use a provisioned throughput model, it is about $6 / hour for one band (this evens out to about the same cost per page if you are saturating the endpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9bdea0-8fec-41f8-bb39-f31a17339970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Example input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32df364d-fc81-425b-8d97-259302c7cd1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![multimodal example.png](./assets/pdf_figure_example.png \"pdf_figure_example.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0944865-8408-47bb-9a7e-07e37ece6f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Example output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "200640e9-a836-46cf-a8d7-47dde79ae1ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## FIGURE 2–5. CYTOKINE BALANCE\n",
    "\n",
    "`<figure>` A diagram showing factors influencing the balance between Th1 and Th2 cytokine responses. The left side lists factors favoring Th1 phenotype, including presence of older siblings, early exposure to day care, tuberculosis, measles, or hepatitis A infection, and rural environment. The right side shows factors favoring Th2 phenotype, such as widespread use of antibiotics, Western lifestyle, urban environment, diet, and sensitization to house-dust mites and cockroaches. The diagram illustrates how these factors shift the balance toward either protective immunity (Th1) or allergic diseases including asthma (Th2). `</figure>`\n",
    "\n",
    "| Factors favoring the Th1 phenotype | Factors favoring the Th2 phenotype |\n",
    "| --- | --- |\n",
    "| Presence of older siblings | Widespread use of antibiotics |\n",
    "| Early exposure to day care | Western lifestyle |\n",
    "| Tuberculosis, measles, or hepatitis A infection | Urban environment |\n",
    "| Rural environment | Diet |\n",
    "|  | Sensitization to house-dust mites and cockroaches |\n",
    "\n",
    "Numerous factors, including alterations in the number or type of infections early in life, the widespread use of antibiotics, adoption of the Western lifestyle, and repeated exposure to allergens, may affect the balance between Th1-type and Th2-type cytokine responses and increase the likelihood that the immune response will be dominated by Th2 cells and thus will ultimately lead to the expression of allergic diseases such as asthma.\n",
    "\n",
    "Reprinted by permission from Busse WW, Lemanske RF. Advances in Immunology N Engl J Med 2001; 344: 350-62. Copyright © 2001 Massachusetts Medical Society. All rights reserved.\n",
    "\n",
    "dramatic increases in asthma prevalence in westernized countries. This hypothesis is based on the assumption that the immune system of the newly born is skewed toward Th2 cytokine generation. Following birth, environmental stimuli such as infections will activate Th1 responses and bring the Th1/Th2 relationship to an appropriate balance. Evidence indicates that the incidence of asthma is reduced in association with certain infections (M. tuberculosis, measles, or hepatitis A), exposure to other children (e.g., presence of older siblings and early enrollment in childcare), and less frequent use of antibiotics (Eder et al. 2006; Gern et al. 1999; Gern and Busse 2002; Horwood et al. 1985; Sears et al. 2003). Furthermore, the absence of these lifestyle events is associated with the persistence of a Th2 cytokine pattern. Under these conditions, the genetic background of the child who has a cytokine imbalance toward Th2 will set the stage to promote the production of IgE antibodies to key environmental antigens, such as house-dust mite, cockroach, Alternaria, and possibly cat. Therefore, a gene-by-environment interaction occurs in which the susceptible host is exposed to environmental factors that are capable of generating IgE, and sensitization occurs. Precisely why the airways of some individuals are susceptible to these allergic events has not been established.\n",
    "\n",
    "There also appears to be a reciprocal interaction between the two subpopulations in which Th1 cytokines can inhibit Th2 generation and vice versa. Allergic inflammation may be the result of an excessive expression of Th2 cytokines. Alternatively, recent studies have suggested the possibility that the loss of normal immune balance arises from a cytokine dysregulation in which Th1 activity in asthma is diminished. The focus on actions of cytokines and chemokines to regulate and activate the inflammatory profile in asthma has provided\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f4f056e-6c87-42bc-9570-7d45a13440c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1) Installations, Imports, and preliminary setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc14a1dd-a266-4ffe-ae89-a2ae594824b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet databricks-sdk httpx PyMuPDF openai\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0616f275-5b7d-4790-b99d-021aba8268ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import fitz \n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random\n",
    "import threading\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql.functions import col, concat, lit, regexp_replace, split\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caaaee6f-d4af-4392-9c6f-c47b1e04ae7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2) Set the UC paths and configuration\n",
    "\n",
    "### To do: update the paths below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3352c61f-e93f-4816-9b7e-ee186a2c5b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UPDATE THESE PATHS FOR YOUR SETUP\n",
    "PDF_DIRECTORY = \"/Volumes/erica/parsing/pdfs/\"  # Directory containing PDFs\n",
    "OUTPUT_CATALOG = \"erica.parsing\"  # Catalog and schema for output tables\n",
    "\n",
    "# You can choose the processing mode:\n",
    "# \"combined\" - All PDFs go into one table with doc_id to distinguish (recommended)\n",
    "# \"separate\" - Each PDF gets its own table\n",
    "PROCESSING_MODE = \"combined\"  # or \"separate\"\n",
    "\n",
    "# Table naming\n",
    "if PROCESSING_MODE == \"combined\":\n",
    "    INTERMEDIATE_TABLE = f\"{OUTPUT_CATALOG}.all_pdfs_parsed_intermediate\"\n",
    "    FINAL_TABLE = f\"{OUTPUT_CATALOG}.all_pdfs_parsed\"\n",
    "else:\n",
    "    # For separate mode, tables will be named dynamically per PDF\n",
    "    pass\n",
    "\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "DATABRICKS_BASE_URL = 'https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/'\n",
    "\n",
    "print(f\"📂 PDF Directory: {PDF_DIRECTORY}\")\n",
    "print(f\"📊 Output Catalog: {OUTPUT_CATALOG}\")\n",
    "print(f\"🔧 Processing Mode: {PROCESSING_MODE}\")\n",
    "if PROCESSING_MODE == \"combined\":\n",
    "    print(f\"💾 Final Table: {FINAL_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcad5de4-3db0-44b9-abec-13abef895813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Count number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1d2a00bc-651f-4c30-8e31-f4f1260da0b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_pdf_pages_fitz(directory=\".\", show_details=True):\n",
    "    \"\"\"\n",
    "    Count pages in all PDF files in a directory using PyMuPDF (fitz)\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory path to scan for PDFs\n",
    "        show_details (bool): Whether to show individual file counts\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (total_pages, file_count, errors)\n",
    "    \"\"\"\n",
    "    pdf_files = list(Path(directory).glob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{directory}'\")\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    total_pages = 0\n",
    "    file_count = 0\n",
    "    errors = 0\n",
    "    \n",
    "    print(f\"Scanning {len(pdf_files)} PDF files in '{directory}'...\\n\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            # Open PDF with fitz\n",
    "            doc = fitz.open(pdf_file)\n",
    "            pages = doc.page_count\n",
    "            doc.close()\n",
    "            \n",
    "            if show_details:\n",
    "                print(f\"{pdf_file.name:<50} {pages:>6} pages\")\n",
    "            \n",
    "            total_pages += pages\n",
    "            file_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR - {pdf_file.name}: {e}\")\n",
    "            errors += 1\n",
    "    print(\"\\n\")\n",
    "    return total_pages, file_count, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f639138-daa4-4654-9f68-4fb69ea0a308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "total_pages, num_documents, errors = (count_pdf_pages_fitz(PDF_DIRECTORY, show_details=True))\n",
    "\n",
    "print(f\"Total {num_documents} PDFs found with a total of {total_pages} pages. {errors} errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f8ad04-ac01-4d77-960f-3a3b8b97d7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#3) Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "055f7776-21a8-4bb6-8dc6-c6f255da4584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63732d80-5ff8-45ad-8c3a-0f1902a82e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_pdf_files(directory_path):\n",
    "    \"\"\"\n",
    "    Get all PDF files from a Unity Catalog volume directory.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to directory containing PDFs\n",
    "        \n",
    "    Returns:\n",
    "        List of PDF file paths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List all files in the directory\n",
    "        files = dbutils.fs.ls(directory_path)\n",
    "        \n",
    "        # Filter for PDF files and clean the paths\n",
    "        pdf_files = []\n",
    "        for file in files:\n",
    "            if file.path.lower().endswith('.pdf'):\n",
    "                # Remove 'dbfs:' prefix if present to work with PyMuPDF\n",
    "                clean_path = file.path.replace('dbfs:', '') if file.path.startswith('dbfs:') else file.path\n",
    "                pdf_files.append(clean_path)\n",
    "        \n",
    "        print(f\"Found {len(pdf_files)} PDF files in {directory_path}\")\n",
    "        for pdf in pdf_files:\n",
    "            file_name = os.path.basename(pdf)\n",
    "            print(f\"  - {file_name}\")\n",
    "            print(f\"    Path: {pdf}\")\n",
    "            \n",
    "        return pdf_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing directory {directory_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def get_clean_doc_name(pdf_path):\n",
    "    \"\"\"Extract a clean document name from the PDF path for table naming.\"\"\"\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    # Remove .pdf extension and clean up for table naming\n",
    "    clean_name = file_name.replace('.pdf', '').replace('.PDF', '')\n",
    "    # Replace special characters with underscores\n",
    "    clean_name = ''.join(c if c.isalnum() else '_' for c in clean_name)\n",
    "    # Remove consecutive underscores and strip\n",
    "    clean_name = '_'.join(filter(None, clean_name.split('_')))\n",
    "    return clean_name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd40e878-b493-44e4-98ca-769c4366824d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_pdf_to_base64(pdf_path, dpi=300):\n",
    "    \"\"\"\n",
    "    PDF conversion with better metadata and error handling.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        dpi: Resolution\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with metadata, success boolean, error message\n",
    "    \"\"\"\n",
    "    \n",
    "    zoom = dpi / 72\n",
    "    zoom_matrix = fitz.Matrix(zoom, zoom)\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        num_pages = len(doc)\n",
    "        \n",
    "        # Extract document metadata\n",
    "        metadata = doc.metadata\n",
    "        file_name = os.path.basename(pdf_path)\n",
    "        clean_doc_name = get_clean_doc_name(pdf_path)\n",
    "        \n",
    "        print(f\"Converting {file_name} to base64: {num_pages} pages at {dpi} DPI...\")\n",
    "        \n",
    "        df_data = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for page_num in range(num_pages):\n",
    "            if page_num % 25 == 0:  # Progress update every 25 pages\n",
    "                print(f\"  Converting page {page_num + 1}/{num_pages} to base64\")\n",
    "            \n",
    "            page = doc.load_page(page_num)\n",
    "            \n",
    "            # Get page dimensions and text for metadata\n",
    "            page_rect = page.rect\n",
    "            page_text_length = len(page.get_text())\n",
    "            \n",
    "            pix = page.get_pixmap(matrix=zoom_matrix, alpha=False)\n",
    "            img_bytes = pix.tobytes(\"png\")  \n",
    "            img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "            \n",
    "            df_data.append({\n",
    "                'doc_id': pdf_path,\n",
    "                'doc_name': clean_doc_name,\n",
    "                'file_name': file_name,\n",
    "                'page_num': page_num + 1,\n",
    "                'total_pages': num_pages,\n",
    "                'page_width': page_rect.width,\n",
    "                'page_height': page_rect.height,\n",
    "                'page_text_length': page_text_length,\n",
    "                'base64_img': img_base64,\n",
    "                'processed_timestamp': datetime.now(),\n",
    "                'dpi': dpi,\n",
    "                'doc_title': metadata.get('title', ''),\n",
    "                'doc_author': metadata.get('author', ''),\n",
    "                'doc_subject': metadata.get('subject', ''),\n",
    "                'doc_creator': metadata.get('creator', '')\n",
    "            })\n",
    "        \n",
    "        doc.close()\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"  Conversion complete: {len(df_data)} pages in {processing_time:.1f}s\")\n",
    "        \n",
    "        return pd.DataFrame(df_data), True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {pdf_path}: {str(e)}\"\n",
    "        print(f\"❌ {error_msg}\")\n",
    "        return None, False, error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b90cbec8-6479-43b5-9ce9-5ebe64b7177e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optional: change `mode=\"append\"` or `mode=\"overwrite\"` in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9abd43f6-715f-4952-b6c7-d9eb0add5f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_to_unity_catalog(df, table_path, mode=\"append\"):\n",
    "    \"\"\"\n",
    "    Save function with better error handling and options.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        \n",
    "        if mode == \"overwrite\":\n",
    "            spark_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(table_path)\n",
    "        else:\n",
    "            spark_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .saveAsTable(table_path)\n",
    "        \n",
    "        print(f\"✅ Saved {len(df)} records to: {table_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving to {table_path}: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2125ec3-989e-4a81-b927-089919e34989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Threadpool with Adaptive Concurrency Functions\n",
    "\n",
    "Handle the LLM processing with adaptive concurrency based on rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b78bbf4-c934-4db4-9ebb-b4952e456d6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RETRYABLE_ERROR_SUBSTRINGS = [\"retry\", \"got empty embedding result\", \"request_limit_exceeded\", \"rate limit\", \"insufficient_quota\", \"expecting value\", \"rate\", \"overloaded\", \"429\", \"bad gateway\", \"502\"]\n",
    "\n",
    "class RateLimitTracker:\n",
    "    \"\"\"Track API rate limits and adjust concurrency dynamically.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_workers=5, min_workers=1, max_workers=10):\n",
    "        self.current_workers = initial_workers\n",
    "        self.min_workers = min_workers\n",
    "        self.max_workers = max_workers\n",
    "        self.rate_limit_events = deque(maxlen=20)  # Track recent rate limits\n",
    "        self.success_count = 0\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def record_rate_limit(self):\n",
    "        \"\"\"Record a rate limit event and potentially reduce workers.\"\"\"\n",
    "        with self.lock:\n",
    "            self.rate_limit_events.append(datetime.now())\n",
    "            \n",
    "            # If we've had multiple rate limits recently, reduce workers\n",
    "            recent_limits = sum(1 for event in self.rate_limit_events \n",
    "                              if datetime.now() - event < timedelta(minutes=2))\n",
    "            \n",
    "            if recent_limits >= 3 and self.current_workers > self.min_workers:\n",
    "                old_workers = self.current_workers\n",
    "                self.current_workers = max(self.min_workers, self.current_workers - 1)\n",
    "                print(f\"🔽 Rate limits detected! Reducing workers: {old_workers} → {self.current_workers}\")\n",
    "                \n",
    "    def record_success(self):\n",
    "        \"\"\"Record successful processing and potentially increase workers.\"\"\"\n",
    "        with self.lock:\n",
    "            self.success_count += 1\n",
    "            \n",
    "            # If no recent rate limits and we've had some successes, gradually increase workers\n",
    "            recent_limits = sum(1 for event in self.rate_limit_events \n",
    "                              if datetime.now() - event < timedelta(minutes=5))\n",
    "            \n",
    "            # Increase workers every 20 successes if no recent rate limits\n",
    "            if (recent_limits == 0 and \n",
    "                self.current_workers < self.max_workers and \n",
    "                self.success_count % 20 == 0):\n",
    "                old_workers = self.current_workers\n",
    "                self.current_workers = min(self.max_workers, self.current_workers + 1)\n",
    "                print(f\"🔼 Performance good! Increasing workers: {old_workers} → {self.current_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f09a8a3f-f10e-4a6c-95a8-30676fdc002f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_single_image(prompt, image_data, image_index, databricks_token, databricks_url, model, rate_tracker):\n",
    "    \"\"\"Process a single image with adaptive rate limiting.\"\"\"\n",
    "    \n",
    "    client = OpenAI(api_key=databricks_token, base_url=databricks_url)\n",
    "    \n",
    "    # Skip empty images\n",
    "    if pd.isna(image_data) or image_data == \"\":\n",
    "        return (image_index, \"ERROR: Empty image\")\n",
    "    \n",
    "    \n",
    "    # Retry logic with exponential backoff\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}\n",
    "                        }\n",
    "                    ]\n",
    "                }]\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content.strip()\n",
    "            rate_tracker.record_success()\n",
    "            \n",
    "            # Print success message if this was a retry attempt\n",
    "            if attempt > 0:\n",
    "                print(f\"✅ SUCCESS: Image {image_index} processed successfully after {attempt + 1} attempts\")\n",
    "            \n",
    "            return (image_index, result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            is_retryable = any(substring in error_str for substring in RETRYABLE_ERROR_SUBSTRINGS)\n",
    "            \n",
    "            if is_retryable:\n",
    "                rate_tracker.record_rate_limit()\n",
    "                \n",
    "                if attempt < 2:  # Only retry if we have attempts left\n",
    "                    # Exponential backoff with jitter\n",
    "                    wait_time = (2 ** attempt) + random.uniform(1, 3)\n",
    "                    print(f\"⚠️  RATE LIMIT: Image {image_index}, attempt {attempt + 1}/3. Retrying in {wait_time:.1f}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"❌ FAILED: Image {image_index} failed after 3 attempts due to rate limiting\")\n",
    "                    return (image_index, f\"ERROR: Rate limited after 3 attempts - {str(e)}\")\n",
    "            else:\n",
    "                print(f\"❌ ERROR: Image {image_index} failed with non-retryable error: {str(e)}\")\n",
    "                return (image_index, f\"ERROR: {str(e)}\")\n",
    "    \n",
    "    return (image_index, \"ERROR: Max retries exceeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a2800b-09d2-4a25-8dda-3cf093de77db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_images_adaptive(prompt, images, databricks_token, databricks_url, \n",
    "                           model=\"databricks-llama-4-maverick\", \n",
    "                           initial_workers=5, min_workers=1, max_workers=10):\n",
    "    \"\"\"\n",
    "    Adaptive processing that adjusts concurrency based on rate limits.\n",
    "    \n",
    "    Args:\n",
    "        images: pandas Series of base64 encoded image strings\n",
    "        databricks_token: Token for Databricks API  \n",
    "        databricks_url: Base URL for Databricks API\n",
    "        model: Model name to use\n",
    "        initial_workers: Starting number of concurrent workers\n",
    "        min_workers: Minimum workers (fallback during heavy rate limiting)\n",
    "        max_workers: Maximum workers (cap for scaling up)\n",
    "        \n",
    "    Returns:\n",
    "        pandas Series: Results with same index as input\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to pandas Series if needed\n",
    "    if not isinstance(images, pd.Series):\n",
    "        images = pd.Series(images)\n",
    "    \n",
    "    results = pd.Series(index=images.index, dtype='object')\n",
    "    rate_tracker = RateLimitTracker(\n",
    "        initial_workers=initial_workers, \n",
    "        min_workers=min_workers, \n",
    "        max_workers=max_workers\n",
    "    )\n",
    "    \n",
    "    print(f\"🚀 Starting transcription of {len(images)} images...\")\n",
    "    print(f\"📊 Model: {model}\")\n",
    "    print(f\"⚙️  Workers: {initial_workers} (range: {min_workers}-{max_workers})\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        with tqdm(total=len(images), desc=\"Processing images\", unit=\"img\") as pbar:\n",
    "            \n",
    "            remaining_items = list(images.items())\n",
    "            \n",
    "            while remaining_items:\n",
    "                # Submit batch based on current worker count\n",
    "                batch_size = min(rate_tracker.current_workers, len(remaining_items))\n",
    "                current_batch = remaining_items[:batch_size]\n",
    "                remaining_items = remaining_items[batch_size:]\n",
    "                \n",
    "                # Submit current batch\n",
    "                futures = {\n",
    "                    executor.submit(process_single_image, prompt, img_data, idx, \n",
    "                                  databricks_token, databricks_url, model, rate_tracker): idx\n",
    "                    for idx, img_data in current_batch\n",
    "                }\n",
    "                \n",
    "                # Process batch results\n",
    "                for future in as_completed(futures):\n",
    "                    try:\n",
    "                        image_index, result = future.result()\n",
    "                        results[image_index] = result\n",
    "                        \n",
    "                        # Update progress bar with status and current worker count\n",
    "                        if result.startswith(\"ERROR:\"):\n",
    "                            pbar.set_postfix({\n",
    "                                \"Last\": f\"❌ {image_index}\", \n",
    "                                \"Workers\": rate_tracker.current_workers,\n",
    "                                \"Rate Limits\": len(rate_tracker.rate_limit_events)\n",
    "                            })\n",
    "                        else:\n",
    "                            pbar.set_postfix({\n",
    "                                \"Last\": f\"✅ {image_index}\", \n",
    "                                \"Workers\": rate_tracker.current_workers,\n",
    "                                \"Rate Limits\": len(rate_tracker.rate_limit_events)\n",
    "                            })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        idx = futures[future]\n",
    "                        results[idx] = f\"ERROR: Exception - {str(e)}\"\n",
    "                        pbar.set_postfix({\n",
    "                            \"Last\": f\"❌ {idx} (Exception)\", \n",
    "                            \"Workers\": rate_tracker.current_workers\n",
    "                        })\n",
    "                        print(f\"❌ EXCEPTION: Image {idx} failed with exception: {str(e)}\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                \n",
    "                # Small delay between batches if we have more to process\n",
    "                if remaining_items:\n",
    "                    time.sleep(0.2)  # Small delay to prevent overwhelming\n",
    "    \n",
    "    # Final summary statistics\n",
    "    error_count = sum(1 for result in results if str(result).startswith(\"ERROR:\"))\n",
    "    success_count = len(results) - error_count\n",
    "    \n",
    "    print(f\"\\n📈 Llama 4 Transcription Summary:\")\n",
    "    print(f\"   ✅ Successful: {success_count}/{len(results)}\")\n",
    "    print(f\"   ❌ Failed: {error_count}/{len(results)}\")\n",
    "    print(f\"   📊 Success rate: {(success_count/len(results)*100):.1f}%\")\n",
    "    print(f\"   🔧 Final worker count: {rate_tracker.current_workers}\")\n",
    "    print(f\"   ⚠️  Total rate limit events: {len(rate_tracker.rate_limit_events)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbdb8f6-4a66-4047-8857-43ab1b0e6537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Main Batch Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d1e3ba-2dc2-4478-8211-0023255f9129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Define the prompt\n",
    "    PROMPT = \"\"\"\n",
    "    Instructions: Transcribe only the visible text from this PDF page. \n",
    "\n",
    "    Rules:\n",
    "    - Use markdown formatting only for text that appears formatted in the original\n",
    "    - Do not add document titles, page headers, or section headings unless explicitly visible\n",
    "    - Do not add introductory text like 'This page contains...' or 'The document shows...' or '# Transcription of PDF Page'\n",
    "    - Preserve exact wording and medical terminology\n",
    "    - For images/diagrams: describe content within <figure></figure> tags\n",
    "    - For tables: use markdown table format if present\n",
    "    - Start transcription immediately without preamble\n",
    "\n",
    "    For visual elements, follow these rules:\n",
    "\n",
    "    **TABLES**: If the content is clearly a structured table, provide BOTH:\n",
    "    1. A detailed caption in <figure></figure> tags describing the table structure and content\n",
    "    2. The actual table recreated in markdown format with proper alignment\n",
    "\n",
    "    **FLOWCHARTS/DECISION TREES**: Provide detailed caption in <figure></figure> tags including:\n",
    "    - Starting point and decision criteria\n",
    "    - All pathways and decision branches\n",
    "    - Specific thresholds, values, and conditions\n",
    "    - Final outcomes and recommendations\n",
    "    - Flow direction and logical connections\n",
    "\n",
    "    **CHARTS/DIAGRAMS**: Provide detailed caption in <figure></figure> tags including:\n",
    "    - Chart type and title\n",
    "    - All categories, sections, and color coding\n",
    "    - Specific values, ranges, and criteria\n",
    "    - Evidence levels and recommendations\n",
    "    - Visual organization and groupings\n",
    "\n",
    "    **FORMS/CHECKLISTS**: Transcribe structure using markdown formatting, preserving:\n",
    "    - Section headers and numbering\n",
    "    - Checkbox options and rating scales\n",
    "    - Please bold the Key in Key-Value Pairs in the form, e.g. **Name **: John Doe.\n",
    "\n",
    "    Preserve exact medical terminology, drug names, dosages, and clinical criteria for diagnostic accuracy.\n",
    "\n",
    "    This transcription will be used for medical diagnosis, so accuracy is critical.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed24713b-aa87-467b-9f96-754df1a6716d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_multiple_pdfs(pdf_directory, output_catalog, prompt=PROMPT, processing_mode=\"combined\", \n",
    "                         dpi=300, model=\"databricks-llama-4-maverick\", initial_workers=5, \n",
    "                         min_workers=1, max_workers=10):\n",
    "    \"\"\"\n",
    "    Process all PDFs in a directory.\n",
    "    \n",
    "    Args:\n",
    "        pdf_directory: Directory containing PDF files\n",
    "        output_catalog: Catalog.schema for output tables\n",
    "        processing_mode: \"combined\" or \"separate\"\n",
    "        dpi: Image resolution\n",
    "        model: LLM model to use\n",
    "    \"\"\"\n",
    "    \n",
    "    # Discover PDF files\n",
    "    pdf_files = get_pdf_files(pdf_directory)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(\"No PDF files found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🚀 Starting batch processing of {len(pdf_files)} PDFs\")\n",
    "    print(f\"📊 Processing mode: {processing_mode}\")\n",
    "    print(f\"🎯 Output catalog: {output_catalog}\")\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    total_files = len(pdf_files)\n",
    "    successful_files = 0\n",
    "    failed_files = 0\n",
    "    total_pages_processed = 0\n",
    "    all_results = []\n",
    "    processing_log = []\n",
    "    \n",
    "    # Process each PDF\n",
    "    for file_idx, pdf_path in enumerate(pdf_files, 1):\n",
    "        file_name = os.path.basename(pdf_path)\n",
    "        clean_doc_name = get_clean_doc_name(pdf_path)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"📄 Processing file {file_idx}/{total_files}: {file_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        file_start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Convert PDF to base64 images\n",
    "            df, success, error = convert_pdf_to_base64(pdf_path, dpi=dpi)\n",
    "            \n",
    "            if not success:\n",
    "                failed_files += 1\n",
    "                processing_log.append({\n",
    "                    'file_name': file_name,\n",
    "                    'status': 'FAILED_CONVERSION',\n",
    "                    'error': error,\n",
    "                    'pages_processed': 0,\n",
    "                    'processing_time': time.time() - file_start_time\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Save intermediate results\n",
    "            if processing_mode == \"combined\":\n",
    "                intermediate_table = f\"{output_catalog}.all_pdfs_parsed_intermediate\"\n",
    "                save_mode = \"append\" if file_idx > 1 else \"overwrite\"\n",
    "            else:\n",
    "                intermediate_table = f\"{output_catalog}.{clean_doc_name}_parsed_intermediate\"\n",
    "                save_mode = \"overwrite\"\n",
    "                \n",
    "            save_to_unity_catalog(df, intermediate_table, mode=save_mode)\n",
    "            \n",
    "            # Process images with LLM\n",
    "            print(f\"🤖 Starting LLM processing for {len(df)} pages...\")\n",
    "            \n",
    "            # Process with adaptive rate limiting\n",
    "            results_series = process_images_adaptive(\n",
    "                prompt=prompt,\n",
    "                images=df['base64_img'],\n",
    "                databricks_token=DATABRICKS_TOKEN,\n",
    "                databricks_url=DATABRICKS_BASE_URL,\n",
    "                model=model,\n",
    "                initial_workers=initial_workers,\n",
    "                min_workers=min_workers,\n",
    "                max_workers=max_workers\n",
    "            )\n",
    "            \n",
    "            # Add transcription results to dataframe\n",
    "            df['transcription'] = results_series\n",
    "            \n",
    "            # Count successful transcriptions\n",
    "            error_count = sum(1 for result in results_series if str(result).startswith(\"ERROR:\"))\n",
    "            success_count = len(results_series) - error_count\n",
    "            \n",
    "            # Save final results\n",
    "            if processing_mode == \"combined\":\n",
    "                final_table = f\"{output_catalog}.all_pdfs_parsed\"\n",
    "                save_mode = \"append\" if file_idx > 1 else \"overwrite\"\n",
    "            else:\n",
    "                final_table = f\"{output_catalog}.{clean_doc_name}_parsed\"\n",
    "                save_mode = \"overwrite\"\n",
    "                \n",
    "            save_success = save_to_unity_catalog(df, final_table, mode=save_mode)\n",
    "            \n",
    "            if save_success:\n",
    "                successful_files += 1\n",
    "                total_pages_processed += len(df)\n",
    "                all_results.append(df)\n",
    "                \n",
    "                file_processing_time = time.time() - file_start_time\n",
    "                \n",
    "                processing_log.append({\n",
    "                    'file_name': file_name,\n",
    "                    'status': 'SUCCESS',\n",
    "                    'pages_processed': len(df),\n",
    "                    'successful_transcriptions': success_count,\n",
    "                    'failed_transcriptions': error_count,\n",
    "                    'processing_time': file_processing_time,\n",
    "                    'final_table': final_table\n",
    "                })\n",
    "                \n",
    "                print(f\"✅ File completed successfully:\")\n",
    "                print(f\"   📊 Pages: {len(df)}\")\n",
    "                print(f\"   ✅ Successful transcriptions: {success_count}\")\n",
    "                print(f\"   ❌ Failed transcriptions: {error_count}\")\n",
    "                print(f\"   ⏱️  Processing time: {file_processing_time:.1f}s\")\n",
    "                print(f\"   💾 Saved to: {final_table}\")\n",
    "            else:\n",
    "                failed_files += 1\n",
    "                processing_log.append({\n",
    "                    'file_name': file_name,\n",
    "                    'status': 'FAILED_SAVE',\n",
    "                    'pages_processed': len(df),\n",
    "                    'processing_time': time.time() - file_start_time\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_files += 1\n",
    "            file_processing_time = time.time() - file_start_time\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            processing_log.append({\n",
    "                'file_name': file_name,\n",
    "                'status': 'FAILED_EXCEPTION',\n",
    "                'error': error_msg,\n",
    "                'pages_processed': 0,\n",
    "                'processing_time': file_processing_time\n",
    "            })\n",
    "            \n",
    "            print(f\"❌ Failed to process {file_name}: {error_msg}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🎊 BATCH PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"📊 Files processed: {successful_files}/{total_files}\")\n",
    "    print(f\"📄 Total pages processed: {total_pages_processed}\")\n",
    "    print(f\"✅ Successful files: {successful_files}\")\n",
    "    print(f\"❌ Failed files: {failed_files}\")\n",
    "    \n",
    "    if processing_mode == \"combined\" and successful_files > 0:\n",
    "        print(f\"💾 All results combined in: {output_catalog}.all_pdfs_parsed\")\n",
    "    \n",
    "    # Show processing log\n",
    "    print(f\"\\n📋 PROCESSING LOG:\")\n",
    "    for log_entry in processing_log:\n",
    "        status_emoji = \"✅\" if log_entry['status'] == 'SUCCESS' else \"❌\"\n",
    "        print(f\"   {status_emoji} {log_entry['file_name']}: {log_entry['status']} \"\n",
    "              f\"({log_entry['pages_processed']} pages, {log_entry['processing_time']:.1f}s)\")\n",
    "        \n",
    "        if 'error' in log_entry:\n",
    "            print(f\"      Error: {log_entry['error']}\")\n",
    "    \n",
    "    return processing_log, all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3274f754-feb1-49ba-93a0-fb79b126d405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4) 🚀 Execute the Batch Processing\n",
    "\n",
    "This cell will start processing all PDFs in your directory.\n",
    "\n",
    "It will first start off with converting the PDFs into base64 images (~150 - 200 pages a min)\n",
    "Then it will call Llama 4 on this images.\n",
    "\n",
    "For pay-per-token model = `databricks-llama-4-maverick`, put initial workers = 5, and max workers at 10.  This processes about 30 pages a minute (depending on the number of tokens per page).\n",
    "\n",
    "For provisioned throughput with 200 model units, put initial workers at 30 and max workers at 40. This processes about 120 pages a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eace50ce-d3f0-48a8-b45c-ff51d54850ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the batch processing\n",
    "processing_log, all_results = process_multiple_pdfs(\n",
    "    pdf_directory=PDF_DIRECTORY,\n",
    "    output_catalog=OUTPUT_CATALOG,\n",
    "    prompt = PROMPT,\n",
    "    processing_mode=PROCESSING_MODE,\n",
    "    dpi=300,\n",
    "    model=\"llama-4-maverick-pt\", #default databricks-llama-4-maverick, change to your own provisioned throughput endpoint for more speed\n",
    "    initial_workers=30, #update if you have a provisioned throughput endpoint\n",
    "    min_workers=1, #default 1\n",
    "    max_workers=50 #update if you have a provisioned throughput endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74e0e03-9b8f-423f-b705-b47674d58e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5) Optional: Query and Inspect Results (PROCESSING_MODE == combined only)\n",
    "\n",
    "Use these cells to explore your processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If using combined mode, show summary statistics\n",
    "if PROCESSING_MODE == \"combined\":\n",
    "    summary_df = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            file_name,\n",
    "            doc_name,\n",
    "            COUNT(*) as total_pages,\n",
    "            SUM(CASE WHEN transcription NOT LIKE 'ERROR:%' THEN 1 ELSE 0 END) as successful_pages,\n",
    "            SUM(CASE WHEN transcription LIKE 'ERROR:%' THEN 1 ELSE 0 END) as failed_pages,\n",
    "            AVG(page_text_length) as avg_page_text_length,\n",
    "            MIN(processed_timestamp) as first_processed,\n",
    "            MAX(processed_timestamp) as last_processed\n",
    "        FROM {FINAL_TABLE}\n",
    "        GROUP BY file_name, doc_name\n",
    "        ORDER BY file_name\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"📊 PROCESSING SUMMARY BY FILE:\")\n",
    "    display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c3d4e5-f6g7-8901-bcde-f23456789012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show sample transcriptions\n",
    "if PROCESSING_MODE == \"combined\":\n",
    "    sample_df = spark.sql(f\"\"\"\n",
    "        SELECT file_name, page_num, LEFT(transcription, 500) as transcription_preview\n",
    "        FROM {FINAL_TABLE}\n",
    "        WHERE transcription NOT LIKE 'ERROR:%'\n",
    "        AND LENGTH(transcription) > 100\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"📝 SAMPLE TRANSCRIPTIONS:\")\n",
    "    display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3d4e5f6-g7h8-9012-cdef-345678901234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show error analysis\n",
    "if PROCESSING_MODE == \"combined\":\n",
    "    error_df = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            file_name,\n",
    "            CASE \n",
    "                WHEN transcription LIKE 'ERROR: Rate limited%' THEN 'Rate Limited'\n",
    "                WHEN transcription LIKE 'ERROR: Empty image%' THEN 'Empty Image'\n",
    "                WHEN transcription LIKE 'ERROR:%' THEN 'Other Error'\n",
    "                ELSE 'Success'\n",
    "            END as result_type,\n",
    "            COUNT(*) as count\n",
    "        FROM {FINAL_TABLE}\n",
    "        GROUP BY file_name, \n",
    "            CASE \n",
    "                WHEN transcription LIKE 'ERROR: Rate limited%' THEN 'Rate Limited'\n",
    "                WHEN transcription LIKE 'ERROR: Empty image%' THEN 'Empty Image'\n",
    "                WHEN transcription LIKE 'ERROR:%' THEN 'Other Error'\n",
    "                ELSE 'Success'\n",
    "            END\n",
    "        ORDER BY file_name, result_type\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"⚠️ ERROR ANALYSIS:\")\n",
    "    display(error_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f6g7h8-i9j0-1234-efgh-567890123456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6) Optional: View individual pages and their transcribed outputs\n",
    "\n",
    "Use this cell to browse individual pages if needed for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6g7h8i9-j0k1-2345-fghi-678901234567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Browse individual images - Update these values to explore\n",
    "BROWSE_FILE_NAME = \"GOLD-2025-Report-v1.0-15Nov2024_WMV.pdf\"  # Change this to your file\n",
    "BROWSE_PAGE_NUM = 40  # Change this to the page you want to see\n",
    "\n",
    "    page_data = spark.sql(f\"\"\"\n",
    "        SELECT base64_img, transcription \n",
    "        FROM {FINAL_TABLE}\n",
    "        WHERE file_name = '{BROWSE_FILE_NAME}' \n",
    "        AND page_num = {BROWSE_PAGE_NUM}\n",
    "        LIMIT 1\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    if page_data:\n",
    "        from IPython.display import Image as IPImage\n",
    "        import base64\n",
    "        \n",
    "        def show_image(base64_str):\n",
    "            return IPImage(data=base64.b64decode(base64_str))\n",
    "        \n",
    "        print(f\"📄 Viewing: {BROWSE_FILE_NAME}, Page {BROWSE_PAGE_NUM}\")\n",
    "        display(show_image(page_data[0]['base64_img']))\n",
    "        \n",
    "        print(page_data[0]['transcription'])\n",
    "    else:\n",
    "        print(f\"❌ No data found for {BROWSE_FILE_NAME}, page {BROWSE_PAGE_NUM}\")\n",
    "else:\n",
    "    print(\"Image browsing only available in 'combined' mode\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2170952444901489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Llama 4 PDF Parsing Notebook #2: Multiple PDFs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
