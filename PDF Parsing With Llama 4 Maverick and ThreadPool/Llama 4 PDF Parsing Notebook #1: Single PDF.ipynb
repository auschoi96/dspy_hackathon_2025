{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "987bffb5-8fc5-4cc5-90d2-7e69435f82f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Single PDF Parsing with LLama 4 Maverick and ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfcad44f-eab9-4ced-9f2a-b86311e5ee8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook processes a single PDF in a Unity Catalog volume and transcribes them using LLama 4 by calling the endpoint using Threadpool and automatically adjusting the number of workers.\n",
    "\n",
    "For examples, speed, and cost estimates, look at Notebook #2 which covers Multi-PDF parsing\n",
    "\n",
    "**Author: Erica Yuen**\n",
    "\n",
    "**Date: 07/03/2025**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ec10ebe-4240-4f6a-8ad7-495de1493f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Installations, Imports, and preliminary setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc14a1dd-a266-4ffe-ae89-a2ae594824b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet databricks-sdk httpx PyMuPDF openai\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0616f275-5b7d-4790-b99d-021aba8268ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import fitz \n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql.functions import col, concat, lit, regexp_replace, split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f4f056e-6c87-42bc-9570-7d45a13440c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Set the UC paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caaaee6f-d4af-4392-9c6f-c47b1e04ae7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###To do: update the paths below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3352c61f-e93f-4816-9b7e-ee186a2c5b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_path = \"/Volumes/erica/parsing/pdfs/Form Example.pdf\"\n",
    "\n",
    "intermediate_table_path = \"erica.parsing.form_parsed_intermediate\"\n",
    "final_table_path = \"erica.parsing.form_parsed\"\n",
    "\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "\n",
    "DATABRICKS_BASE_URL = 'https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a554676-844e-484f-93f6-dcc83fc05411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dbruntime.databricks_repl_context import get_context\n",
    "\n",
    "get_context().workspaceId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25f8ad04-ac01-4d77-960f-3a3b8b97d7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Parsing the documents and figure captions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2125ec3-989e-4a81-b927-089919e34989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Convert PDF to Base64 Image\n",
    "\n",
    "This takes about **2 min for 440 pages**\n",
    "\n",
    "Update save_to_unity_catalog if you don't want overrwrites:\n",
    "\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "63732d80-5ff8-45ad-8c3a-0f1902a82e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import fitz \n",
    "import pandas as pd\n",
    "\n",
    "def convert_pdf_to_base64(pdf_path, dpi=300):\n",
    "    \"\"\"\n",
    "    Convert PDF to base64 images optimized for RAG applications.\n",
    "    Simple single-threaded approach for reliable performance.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        dpi: Resolution\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with columns: page_num, base64_img, doc_id\n",
    "    \"\"\"\n",
    "    \n",
    "    zoom = dpi / 72\n",
    "    zoom_matrix = fitz.Matrix(zoom, zoom)\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "    num_pages = len(doc)\n",
    "    \n",
    "    print(f\"Processing {num_pages} pages at {dpi} DPI...\")\n",
    "    \n",
    "    df_data = []\n",
    "    \n",
    "    for page_num in range(num_pages):\n",
    "        if page_num % 25 == 0:  # Progress update every 25 pages\n",
    "            print(f\"Processing page {page_num + 1}/{num_pages}\")\n",
    "        \n",
    "        page = doc.load_page(page_num)\n",
    "        \n",
    "        pix = page.get_pixmap(matrix=zoom_matrix, alpha=False)\n",
    "        img_bytes = pix.tobytes(\"png\")  \n",
    "        img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "        \n",
    "        df_data.append({\n",
    "            'page_num': page_num + 1,\n",
    "            'base64_img': img_base64,\n",
    "            'doc_id': pdf_path\n",
    "        })\n",
    "    \n",
    "    doc.close()\n",
    "    print(f\"Conversion complete. Generated {len(df_data)} base64 images.\")\n",
    "    \n",
    "    return pd.DataFrame(df_data)\n",
    "\n",
    "def save_to_unity_catalog(df, table_path = intermediate_table_path):\n",
    "    \"\"\"\n",
    "    Save the DataFrame to Unity Catalog\n",
    "    \"\"\"\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    \n",
    "    spark_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(table_path)\n",
    "    \n",
    "    print(f\"Saved to Unity Catalog: {table_path}\")\n",
    "    return spark_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd40e878-b493-44e4-98ca-769c4366824d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = convert_pdf_to_base64(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9abd43f6-715f-4952-b6c7-d9eb0add5f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "print(len(df))\n",
    "\n",
    "save_to_unity_catalog(df, table_path = intermediate_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcbdb8f6-4a66-4047-8857-43ab1b0e6537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#View images - Update PAGE_NUM to browse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed24713b-aa87-467b-9f96-754df1a6716d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image as IPImage\n",
    "\n",
    "\n",
    "PAGE_NUM = 1\n",
    "\n",
    "def show_image(base64_str):\n",
    "    return IPImage(data=base64.b64decode(base64_str))\n",
    "\n",
    "show_image(df.iloc[PAGE_NUM - 1]['base64_img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f09a8a3f-f10e-4a6c-95a8-30676fdc002f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Transcribe the follow form into markdown. \n",
    "Please bold all keys in key value pairs, and output sections with section headers. \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0b78bbf4-c934-4db4-9ebb-b4952e456d6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "RETRYABLE_ERROR_SUBSTRINGS = [\"retry\", \"got empty embedding result\", \"request_limit_exceeded\", \"rate limit\", \"insufficient_quota\", \"expecting value\", \"rate\", \"overloaded\", \"429\", \"bad gateway\", \"502\"]\n",
    "\n",
    "class RateLimitTracker:\n",
    "    \"\"\"Track API rate limits and adjust concurrency dynamically.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_workers=5, min_workers=1, max_workers=10):\n",
    "        self.current_workers = initial_workers\n",
    "        self.min_workers = min_workers\n",
    "        self.max_workers = max_workers\n",
    "        self.rate_limit_events = deque(maxlen=20)  # Track recent rate limits\n",
    "        self.success_count = 0\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def record_rate_limit(self):\n",
    "        \"\"\"Record a rate limit event and potentially reduce workers.\"\"\"\n",
    "        with self.lock:\n",
    "            self.rate_limit_events.append(datetime.now())\n",
    "            \n",
    "            # If we've had multiple rate limits recently, reduce workers\n",
    "            recent_limits = sum(1 for event in self.rate_limit_events \n",
    "                              if datetime.now() - event < timedelta(minutes=2))\n",
    "            \n",
    "            if recent_limits >= 3 and self.current_workers > self.min_workers:\n",
    "                old_workers = self.current_workers\n",
    "                self.current_workers = max(self.min_workers, self.current_workers - 1)\n",
    "                print(f\"ðŸ”½ Rate limits detected! Reducing workers: {old_workers} â†’ {self.current_workers}\")\n",
    "                \n",
    "    def record_success(self):\n",
    "        \"\"\"Record successful processing and potentially increase workers.\"\"\"\n",
    "        with self.lock:\n",
    "            self.success_count += 1\n",
    "            \n",
    "            # If no recent rate limits and we've had some successes, gradually increase workers\n",
    "            recent_limits = sum(1 for event in self.rate_limit_events \n",
    "                              if datetime.now() - event < timedelta(minutes=5))\n",
    "            \n",
    "            # Increase workers every 20 successes if no recent rate limits\n",
    "            if (recent_limits == 0 and \n",
    "                self.current_workers < self.max_workers and \n",
    "                self.success_count % 20 == 0):\n",
    "                old_workers = self.current_workers\n",
    "                self.current_workers = min(self.max_workers, self.current_workers + 1)\n",
    "                print(f\"ðŸ”¼ Performance good! Increasing workers: {old_workers} â†’ {self.current_workers}\")\n",
    "\n",
    "def process_single_image(prompt, image_data, image_index, databricks_token, databricks_url, model, rate_tracker):\n",
    "    \"\"\"Process a single image with adaptive rate limiting.\"\"\"\n",
    "    \n",
    "    client = OpenAI(api_key=databricks_token, base_url=databricks_url)\n",
    "    \n",
    "    # Skip empty images\n",
    "    if pd.isna(image_data) or image_data == \"\":\n",
    "        return (image_index, \"ERROR: Empty image\")\n",
    "    \n",
    "    \n",
    "    # Retry logic with exponential backoff\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}\n",
    "                        }\n",
    "                    ]\n",
    "                }]\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content.strip()\n",
    "            rate_tracker.record_success()\n",
    "            \n",
    "            # Print success message if this was a retry attempt\n",
    "            if attempt > 0:\n",
    "                print(f\"âœ… SUCCESS: Image {image_index} processed successfully after {attempt + 1} attempts\")\n",
    "            \n",
    "            return (image_index, result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            is_retryable = any(substring in error_str for substring in RETRYABLE_ERROR_SUBSTRINGS)\n",
    "            \n",
    "            if is_retryable:\n",
    "                rate_tracker.record_rate_limit()\n",
    "                \n",
    "                if attempt < 2:  # Only retry if we have attempts left\n",
    "                    # Exponential backoff with jitter\n",
    "                    wait_time = (2 ** attempt) + random.uniform(1, 3)\n",
    "                    print(f\"âš ï¸  RATE LIMIT: Image {image_index}, attempt {attempt + 1}/3. Retrying in {wait_time:.1f}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"âŒ FAILED: Image {image_index} failed after 3 attempts due to rate limiting\")\n",
    "                    return (image_index, f\"ERROR: Rate limited after 3 attempts - {str(e)}\")\n",
    "            else:\n",
    "                print(f\"âŒ ERROR: Image {image_index} failed with non-retryable error: {str(e)}\")\n",
    "                return (image_index, f\"ERROR: {str(e)}\")\n",
    "    \n",
    "    return (image_index, \"ERROR: Max retries exceeded\")\n",
    "\n",
    "def process_images_adaptive(prompt, images, databricks_token, databricks_url, \n",
    "                           model=\"databricks-llama-4-maverick\", \n",
    "                           initial_workers=5, min_workers=1, max_workers=10):\n",
    "    \"\"\"\n",
    "    Adaptive processing that adjusts concurrency based on rate limits.\n",
    "    \n",
    "    Args:\n",
    "        images: pandas Series of base64 encoded image strings\n",
    "        databricks_token: Token for Databricks API  \n",
    "        databricks_url: Base URL for Databricks API\n",
    "        model: Model name to use\n",
    "        initial_workers: Starting number of concurrent workers\n",
    "        min_workers: Minimum workers (fallback during heavy rate limiting)\n",
    "        max_workers: Maximum workers (cap for scaling up)\n",
    "        \n",
    "    Returns:\n",
    "        pandas Series: Results with same index as input\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to pandas Series if needed\n",
    "    if not isinstance(images, pd.Series):\n",
    "        images = pd.Series(images)\n",
    "    \n",
    "    results = pd.Series(index=images.index, dtype='object')\n",
    "    rate_tracker = RateLimitTracker(\n",
    "        initial_workers=initial_workers, \n",
    "        min_workers=min_workers, \n",
    "        max_workers=max_workers\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸš€ Starting adaptive processing of {len(images)} images...\")\n",
    "    print(f\"ðŸ“Š Model: {model}\")\n",
    "    print(f\"âš™ï¸  Workers: {initial_workers} (range: {min_workers}-{max_workers})\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        with tqdm(total=len(images), desc=\"Processing images\", unit=\"img\") as pbar:\n",
    "            \n",
    "            remaining_items = list(images.items())\n",
    "            \n",
    "            while remaining_items:\n",
    "                # Submit batch based on current worker count\n",
    "                batch_size = min(rate_tracker.current_workers, len(remaining_items))\n",
    "                current_batch = remaining_items[:batch_size]\n",
    "                remaining_items = remaining_items[batch_size:]\n",
    "                \n",
    "                # Submit current batch\n",
    "                futures = {\n",
    "                    executor.submit(process_single_image, prompt, img_data, idx, \n",
    "                                  databricks_token, databricks_url, model, rate_tracker): idx\n",
    "                    for idx, img_data in current_batch\n",
    "                }\n",
    "                \n",
    "                # Process batch results\n",
    "                for future in as_completed(futures):\n",
    "                    try:\n",
    "                        image_index, result = future.result()\n",
    "                        results[image_index] = result\n",
    "                        \n",
    "                        # Update progress bar with status and current worker count\n",
    "                        if result.startswith(\"ERROR:\"):\n",
    "                            pbar.set_postfix({\n",
    "                                \"Last\": f\"âŒ {image_index}\", \n",
    "                                \"Workers\": rate_tracker.current_workers,\n",
    "                                \"Rate Limits\": len(rate_tracker.rate_limit_events)\n",
    "                            })\n",
    "                        else:\n",
    "                            pbar.set_postfix({\n",
    "                                \"Last\": f\"âœ… {image_index}\", \n",
    "                                \"Workers\": rate_tracker.current_workers,\n",
    "                                \"Rate Limits\": len(rate_tracker.rate_limit_events)\n",
    "                            })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        idx = futures[future]\n",
    "                        results[idx] = f\"ERROR: Exception - {str(e)}\"\n",
    "                        pbar.set_postfix({\n",
    "                            \"Last\": f\"âŒ {idx} (Exception)\", \n",
    "                            \"Workers\": rate_tracker.current_workers\n",
    "                        })\n",
    "                        print(f\"âŒ EXCEPTION: Image {idx} failed with exception: {str(e)}\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                \n",
    "                # Small delay between batches if we have more to process\n",
    "                if remaining_items:\n",
    "                    time.sleep(0.2)  # Small delay to prevent overwhelming\n",
    "    \n",
    "    # Final summary statistics\n",
    "    error_count = sum(1 for result in results if str(result).startswith(\"ERROR:\"))\n",
    "    success_count = len(results) - error_count\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ ADAPTIVE PROCESSING SUMMARY:\")\n",
    "    print(f\"   âœ… Successful: {success_count}/{len(results)}\")\n",
    "    print(f\"   âŒ Failed: {error_count}/{len(results)}\")\n",
    "    print(f\"   ðŸ“Š Success rate: {(success_count/len(results)*100):.1f}%\")\n",
    "    print(f\"   ðŸ”§ Final worker count: {rate_tracker.current_workers}\")\n",
    "    print(f\"   âš ï¸  Total rate limit events: {len(rate_tracker.rate_limit_events)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1299bc27-1302-45c1-842a-1898d0ab9431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸš€ #Execute PDF Parsing \n",
    "\n",
    "Update the initial workers if you are using an endpoint other than the default `model=\"databricks-llama-4-maverick\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a2800b-09d2-4a25-8dda-3cf093de77db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_series = process_images_adaptive(\n",
    "    prompt = prompt,\n",
    "    images=df['base64_img'],\n",
    "    databricks_token=DATABRICKS_TOKEN,\n",
    "    databricks_url=DATABRICKS_BASE_URL,\n",
    "    model=\"databricks-llama-4-maverick\",\n",
    "    initial_workers=5,  # Start with 5 workers for Pay-Per-Token, change to 30 if Provisioned Throughput with 200 model units\n",
    "    min_workers=1,      # Fall back to 1 if heavy rate limiting, change to 10 if Provisioned Throughput with 200 model units\n",
    "    max_workers=10      # Scale up to 10 if performance is good, change to 40 if Provisioned Throughput with 200 model units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eace50ce-d3f0-48a8-b45c-ff51d54850ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['transcription'] = results_series\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74e0e03-9b8f-423f-b705-b47674d58e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_to_unity_catalog(df, table_path = final_table_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2170952444901489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Llama 4 PDF Parsing Notebook #1: Single PDF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
