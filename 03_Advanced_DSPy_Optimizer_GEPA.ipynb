{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f103fffc-ff76-4766-b1bc-a8c0f3d3a388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Advanced DSPy Integrations\n",
    "\n",
    "In this section (and mainly because it does not nicely fit anywhere), you will learn about the following:\n",
    "\n",
    "1. DSPy optimizers\n",
    "2. DSPy's MCP Integration \n",
    "3. DSPy with Databricks AI Bridge  \n",
    "4. DSPy with ai_query\n",
    "\n",
    "Use each of these capabilities as necessary for your use case. These are not required to successfully build with DSPy but help when building on Databricks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9562ab35-7815-41a4-977e-4cbe543b137e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#DSPy Prompt Optimizers \n",
    "\n",
    "Iterating through prompts manually is tedious. Without an automated and grounded/objective way of iterating development of prompts, it becomes nearly impossible to maintain prompts over time, especially post production. \n",
    "\n",
    "**Automated improvement** - Instead of manually tweaking prompts through trial-and-error, DSPy systematically optimizes them based on your metrics and training examples. This can save significant time and often discovers better prompts than manual engineering.\n",
    "\n",
    "**Data-driven optimization** - The optimizers learn from your specific examples and use cases, tailoring prompts to your actual needs rather than generic best practices.\n",
    "Complex pipeline optimization - When you have multi-step LLM workflows (retrieval → reasoning → generation), DSPy can optimize the entire pipeline together, which is much harder to do manually.\n",
    "\n",
    "**Reproducible and systematic** - Unlike ad-hoc prompt engineering, DSPy provides a programmatic, repeatable process for improving your LLM applications.\n",
    "\n",
    "**Handling prompt brittleness** - Optimizers can find more robust prompts that work across different examples, reducing the brittleness common with hand-crafted prompts. \n",
    "\n",
    "DSPy optimizers are particularly useful when you:\n",
    "\n",
    "1. Have clear metrics and evaluation data\n",
    "2. Need to optimize complex, multi-step LLM pipelines\n",
    "3. Want to adapt prompts for different models (DSPy can re-optimize when you switch models)\n",
    "4. Have spent significant time manually tweaking prompts without great results\n",
    "5. Need consistent performance across diverse inputs\n",
    "\n",
    "##Cost Value of Prompt Optimizers \n",
    "\n",
    "The Databricks Mosaic AI Research Team released a blog post highlighting how they achieve 90x cost savings by using GEPA, a prompt optimizer on their AI workflows. It highlights how we can find significant performance gains just from optimizing prompts on smaller LLMs. Check out the blog here: https://www.databricks.com/blog/building-state-art-enterprise-agents-90x-cheaper-automated-prompt-optimization\n",
    "\n",
    "If costs are stopping you from going to production, it is essentially mandatory to do prompt optimization so that you are enabled to use smaller LLMs. \n",
    "\n",
    "For example, below is a cost breakdown of using GPT-OSS 20B vs Claude Sonnet 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc63b2b9-47e5-47ee-b4f0-713f84a9eaf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Prompt Optimization Demo: GEPA \n",
    "\n",
    "We will use the same optimizer using in the Mosaic AI research blog post to highlight how powerful optimizers are. In this section, you will optimize a GPT OSS 20B model with a Claude Sonnet 4.5 model as the teacher LLM. We will compare the 20B model to the 120B and Claude to see how well 20B does pre and post optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f34d208-df14-4556-87b0-ff249f388078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade dspy mlflow databricks-agents\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92932032-81e3-4709-b6eb-165202e67211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set up data\n",
    "The following downloads the [pubmed text classification cased](https://huggingface.co/datasets/ml4pubmed/pubmed-text-classification-cased/resolve/main/{}.csv) dataset from Huggingface and writes a utility to ensure that your train and test split has the same labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43900158-8671-4e68-802a-cc125bffed03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dspy.datasets.dataset import Dataset\n",
    "from pandas import StringDtype\n",
    "\n",
    "def read_data_and_subset_to_categories() -> tuple[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Read the pubmed-text-classification-cased dataset. Docs can be found in the url below:\n",
    "    https://huggingface.co/datasets/ml4pubmed/pubmed-text-classification-cased/resolve/main/{}.csv\n",
    "    \"\"\"\n",
    "\n",
    "    # Read train/test split\n",
    "    file_path = \"https://huggingface.co/datasets/ml4pubmed/pubmed-text-classification-cased/resolve/main/{}.csv\"\n",
    "    train = pd.read_csv(file_path.format(\"train\"))\n",
    "    test = pd.read_csv(file_path.format(\"test\"))\n",
    "\n",
    "    train.drop('description_cln', axis=1, inplace=True)\n",
    "    test.drop('description_cln', axis=1, inplace=True)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, n_train_per_label: int = 40, n_test_per_label: int = 20, *args, **kwargs\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_train_per_label = n_train_per_label\n",
    "        self.n_test_per_label = n_test_per_label\n",
    "\n",
    "        self._create_train_test_split_and_ensure_labels()\n",
    "\n",
    "    def _create_train_test_split_and_ensure_labels(self) -> None:\n",
    "        \"\"\"Perform a train/test split that ensure labels in `test` are also in `train`.\"\"\"\n",
    "        # Read the data\n",
    "        train_df, test_df = read_data_and_subset_to_categories()\n",
    "\n",
    "        train_df = train_df.astype(StringDtype())\n",
    "        test_df = test_df.astype(StringDtype())\n",
    "\n",
    "        # Sample for each label\n",
    "        train_samples_df = pd.concat([\n",
    "            group.sample(n=self.n_train_per_label, random_state=1) \n",
    "            for _, group in train_df.groupby('target')\n",
    "        ])\n",
    "        test_samples_df = pd.concat([\n",
    "            group.sample(n=self.n_test_per_label, random_state=1) \n",
    "            for _, group in test_df.groupby('target')\n",
    "        ])\n",
    "\n",
    "        # Set DSPy class variables\n",
    "        self._train = train_samples_df.to_dict(orient=\"records\")\n",
    "        self._test = test_samples_df.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# Sample a train/test split from the pubmed-text-classification-cased dataset\n",
    "dataset = CSVDataset(n_train_per_label=3, n_test_per_label=10)\n",
    "\n",
    "# Create train and test sets containing DSPy examples\n",
    "train_dataset = [example.with_inputs(\"description\") for example in dataset.train]\n",
    "test_dataset = [example.with_inputs(\"description\") for example in dataset.test]\n",
    "\n",
    "print(f\"train dataset size: \\n {len(train_dataset)}\")\n",
    "print(f\"test dataset size: \\n {len(test_dataset)}\")\n",
    "print(f\"Train labels: \\n {set([example.target for example in dataset.train])}\")\n",
    "print(f\"Sample entry: \\n {train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03e71338-11bc-42ac-bfde-be6c6e59c096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Set up the DSPy module and signature for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b3709fb-36e9-48f3-8504-7f5792e18d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import mlflow\n",
    "import dspy\n",
    "\n",
    "# turning on autologging traces\n",
    "mlflow.dspy.autolog(\n",
    "    log_evals=True,\n",
    "    log_compiles=True,\n",
    "    log_traces_from_compile=True\n",
    ")\n",
    "\n",
    "# Create a signature for the DSPy module\n",
    "class TextClassificationSignature(dspy.Signature):\n",
    "    description: str = dspy.InputField()\n",
    "    target: Literal[\n",
    "        'CONCLUSIONS', 'RESULTS', 'METHODS', 'OBJECTIVE', 'BACKGROUND'\n",
    "        ] = dspy.OutputField()\n",
    "\n",
    "\n",
    "class TextClassifier(dspy.Module):\n",
    "    \"\"\"\n",
    "    Classifies medical texts into a previously defined set of categories.\n",
    "    \"\"\"\n",
    "    def __init__(self, lm_name: str):\n",
    "        super().__init__()\n",
    "        # Define the language model\n",
    "        self.lm = dspy.LM(model=f\"databricks/{lm_name}\", max_tokens = 25000, cache=False, reasoning_effort=\"medium\")\n",
    "        # Define the prediction strategy\n",
    "        self.generate_classification = dspy.Predict(TextClassificationSignature)\n",
    "\n",
    "    def forward(self, description: str):\n",
    "        \"\"\"Returns the predcited category of the description text provided\"\"\"\n",
    "        with dspy.context(lm=self.lm):\n",
    "            return self.generate_classification(description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2517d37-bea6-4f89-81bb-d20ddbc0cd2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Let's test that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c658fb5-ad3e-47a9-8279-0c9e6543ff1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initilize our impact_improvement class\n",
    "text_classifier = TextClassifier(lm_name=\"databricks-gpt-oss-20b\")\n",
    "\n",
    "print(\n",
    "  text_classifier(description=\"This study is designed as a randomised controlled trial in which men living with HIV in Australia will be assigned to either an intervention group or usual care control group .\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e2a60d7-46e7-4023-8c3c-f384bc6168d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Make an Evaluation Function\n",
    "\n",
    "Now we need an evaluation function to ensure that we provide correct feedback to guide the models in the right direction. GEPA accepts numeric and text feedback which allows us to integrate AI Judges. AI judges enable us to dynamically react to the performance of the smaller language model and provide more direct, relevant feedback, especially when the AI judge is grounded in our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86fa8b8-853b-493b-9f39-0a67bd9768a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from databricks.agents.evals import judges\n",
    "\n",
    "def validate_classification_with_feedback(example, prediction, trace=None, pred_name=None, pred_trace=None) -> bool:\n",
    "    \"\"\"\n",
    "    Uses Dtabricks AI judges to validate the prediction and return score (1.0 = corract, 0.0 = incorrect) plus feedback.\n",
    "    \"\"\"\n",
    "    # Call correctness judge \n",
    "    judgement = judges.correctness(\n",
    "        request=example.description,\n",
    "        response=prediction.target,\n",
    "        expected_response=example.target\n",
    "    )\n",
    "    # obtain score from judgement (1.0 = correct, 0.0 = incorrect)\n",
    "    if judgement and judgement.value: \n",
    "        score = int(judgement.value.name == \"YES\")\n",
    "    else:\n",
    "        # if no judgement, fallback to comparing prediction to expected\n",
    "        score = int(example.target == prediction.target)\n",
    "\n",
    "    # obtain feedback from judgement\n",
    "    if judgement and judgement.rationale:\n",
    "        feedback = judgement.rationale\n",
    "    else:\n",
    "        # if no judgement, do not provide feedback  \n",
    "        feedback = None\n",
    "    return dspy.Prediction(score=score, feedback=feedback)\n",
    "\n",
    "def check_accuracy(classifier, test_data: pd.DataFrame = test_dataset) -> float:\n",
    "    \"\"\"\n",
    "    Checks the accuracy of the classifier on the test data.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for example in test_data:\n",
    "        prediction = classifier(description=example[\"description\"])\n",
    "        score = validate_classification_with_feedback(example, prediction).score\n",
    "        scores.append(score)\n",
    "        \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd036cb9-66b2-485c-9cb8-7a98f8adb0d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Let's test GPT-OSS 20B with this Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dea523f-6662-4220-886f-6ec01c6d61da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "small_lm_name = \"databricks-gpt-oss-20b\"\n",
    "uncompiled_small_lm_accuracy = check_accuracy(TextClassifier(lm_name=small_lm_name))\n",
    "\n",
    "displayHTML(f\"<h1>Uncompiled {small_lm_name} accuracy: {uncompiled_small_lm_accuracy}</h1>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e2ed0a8-0562-4065-b699-f971036b0c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Now let's test GPT-OSS 120B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "719046a4-ee67-4947-b679-b30078a9eef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lager_lm_name = \"databricks-gpt-oss-120b\"\n",
    "uncompiled_large_lm_accuracy = check_accuracy(TextClassifier(lm_name=lager_lm_name))\n",
    "\n",
    "displayHTML(f\"<h1>Uncompiled {lager_lm_name} accuracy: {uncompiled_large_lm_accuracy}</h1>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd0c09b9-75ce-4774-bbd3-15d448ca7bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Now Claude's turn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7498ed37-2092-419a-b6de-0ac197b7ca7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lager_lm_name = \"databricks-claude-sonnet-4\"\n",
    "uncompiled_large_lm_accuracy = check_accuracy(TextClassifier(lm_name=lager_lm_name))\n",
    "\n",
    "displayHTML(f\"<h1>Uncompiled {lager_lm_name} accuracy: {uncompiled_large_lm_accuracy}</h1>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6fcf8d5-4804-41b5-9385-2b3a51696a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### We can see that 20B just cannot compete with the bigger, frontier models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf19c65-fcdd-4272-aa9f-537a8e43e743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Time to run GEPA\n",
    "\n",
    "Now we have our baseline with the larger, frontier models. We can optimize GPT-OSS 20B to see how well it does compared to the larger, frontier models. \n",
    "\n",
    "If you need to read more about GEPA, check out the resources here: \n",
    "1. GEPA Paper: https://arxiv.org/pdf/2507.19457 \n",
    "2. DSPy GEPA Tutorials: https://dspy.ai/api/optimizers/GEPA/overview/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7acc5a66-6ff4-4478-9efa-f2644e6e3315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# defining an UUID to identify the optimized module\n",
    "id = str(uuid.uuid4())\n",
    "print(f\"id: {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6b42a6-6534-46ab-aa5b-bd170cc7e823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "small_lm_name = \"databricks-gpt-oss-20b\"\n",
    "reflection_lm_name = \"databricks-claude-sonnet-4\"\n",
    "\n",
    "gepa = dspy.GEPA(\n",
    "    metric=validate_classification_with_feedback,\n",
    "    auto=\"light\",\n",
    "    reflection_minibatch_size=15,\n",
    "    reflection_lm=dspy.LM(f\"databricks/{reflection_lm_name}\", max_tokens=25000),\n",
    "    num_threads=16,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"gepa_{id}\"):\n",
    "    compiled_gepa = gepa.compile(\n",
    "        TextClassifier(lm_name=small_lm_name),\n",
    "        trainset=train_dataset, #reminder: Only passing 15 training sets! \n",
    "    )\n",
    "\n",
    "compiled_gepa.save(f\"compiled_gepa_{id}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aa6583b-0ece-458e-ace7-eda09c5f951a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Let's try it again\n",
    "\n",
    "You can see the optimized prompt is saved as a json. We can load this json and use this with a model. Let's try this again with GPT-OSS 20B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "737bd632-b58b-4a4b-8bae-d380221d7dc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_classifier_gepa = TextClassifier(lm_name=small_lm_name)\n",
    "text_classifier_gepa.load(f\"compiled_gepa_{id}.json\")\n",
    "\n",
    "compiled_small_lm_accuracy = check_accuracy(text_classifier_gepa)\n",
    "displayHTML(f\"<h1>Compiled {small_lm_name} accuracy: {compiled_small_lm_accuracy}</h1>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fc19fd0-4701-4c76-8a1a-d1b6e05fda18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Look at that score! \n",
    "\n",
    "We managed to improve GPT-OSS 20B's performance by 12 points, beating GPT-OSS 120B. \n",
    "\n",
    "If this was your use case, you may be more comfortable deploying GPT-OSS 20B instead of Claude Sonnet, which is about 60x to 75x to use than Claude Sonnet. \n",
    "\n",
    "Now you have some options. \n",
    "1. You use a model that is 60x to 75x cheaper and faster in latency than Claude 4 Sonnet at the cost of 6 points if that's acceptable. \n",
    "2. You use a model that is 20x to 22x cheaper, faster in latency AND BEATS GPT-OSS 120B. This may make hosting the model more achievable \n",
    "\n",
    "Ideally in a production use case, you will want to host your model. Now you have more wiggle room in doing so! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39897a2c-1ed4-4504-9900-d998c18be3cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#You can inspect the prompt below! \n",
    "\n",
    "It's not a significant change from what we started with but we now have an automated way to find huge gains in performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c404ff13-bbd5-4a03-979d-bf61be6ecef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(text_classifier_gepa.lm.history[-1][\"messages\"][0][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d3500f3-9c2a-4e39-93ad-bef61f406ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_65bc13ea-276c-4905-a728-9fe2fb1780e2",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_Advanced_DSPy_Optimizer_GEPA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
